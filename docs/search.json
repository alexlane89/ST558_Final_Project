[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA Quarto File",
    "section": "",
    "text": "This is the initial Exploratory Data Analysis file which will examine the Diabetes Health Indicators data set.\n\n\n\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) conducts yearly phone surveys in the United States. The surveys include questions for Americans about their health status and care choices. Our data set is a subset of the BRFSS survey calls from 2015 which focus on diabetes-related survey responses. Below is a brief description of each of the variables included in the data set.\n\nDiabetes_binary: The dependent variable. A 0/1 associated with No/Yes response to “have you been told you have diabetes (or prediabetes)?\nHighBP: Predictor indicating an individual has been told they have high blood pressure.\nHighChol: Predictor indicating an individual has been told they have high cholesterol.\nCholCheck: Cholesterol check has occurred within the past 5 years.\nBMI: Body Mass Index; calculated as a ratio of weight to height.\nSmoker: Response to question, “Have you smoked at least 100 cigarettes in your entire life?\nStroke: Predictor indicating an individual has been told they experienced a stroke.\nHeartDiseaseorAttack: Individual has experienced heart disease or myocardial infarction.\nPhysActivity: Respondent has engaged in physical activity in the past 30 days.\nFruits: Respondent consumes fruit once or more per day.\nVeggies: Respondent consumes vegetables once or more per day.\nHvyAlcoholConsump: More than 14 drinks per week for men & more than 7 drinks per week for women.\nAnyHealthcare: Respondent has some kind of healtch coverage.\nNoDocbcCost: Response to the question, “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\nGenHlth: General scale (1-5) assessment of health\nMentHlth: Response to the question, “How many days during the past 30 was your mental health not good?”\nPhysHlth: Response to the question, “How many days during the past 30 was your physical health not good?”\nDiffWalk: Response to the question, “Do you have serious difficulty walking or climbing stairs?”\nSex: Sex identification 0/1::female/male\nAge: 13-level age category starting at 18 (i.e. level 1 is 18-24)\nEducation: Education scale 1-6 &gt; 1: Never attended school or only kindergarten &gt; 2: Grades 1-8 (elementary) &gt; 3: Grades 9-11 (some high school) &gt; 4: Grade 12 or GED (High school graduate) &gt; 5: College 1 to 3 years (some college) &gt; 6: College 4 years or more (college graduate)\nIncome: Income scale 1-8, ranging from 1 = less than 10,000 to 8 = 75,000 or more.\n\n\n\nI will begin my data exploration by activiting the necessary packages\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nWe will read in the data to a dataframe object, and purposefully identify the data type for each of the 22 variables.\n\n\ndiabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspec(diabetes)\n\ncols(\n  Diabetes_binary = col_double(),\n  HighBP = col_double(),\n  HighChol = col_double(),\n  CholCheck = col_double(),\n  BMI = col_double(),\n  Smoker = col_double(),\n  Stroke = col_double(),\n  HeartDiseaseorAttack = col_double(),\n  PhysActivity = col_double(),\n  Fruits = col_double(),\n  Veggies = col_double(),\n  HvyAlcoholConsump = col_double(),\n  AnyHealthcare = col_double(),\n  NoDocbcCost = col_double(),\n  GenHlth = col_double(),\n  MentHlth = col_double(),\n  PhysHlth = col_double(),\n  DiffWalk = col_double(),\n  Sex = col_double(),\n  Age = col_double(),\n  Education = col_double(),\n  Income = col_double()\n)\n\n\n\nAll data types are “doubles”, but for some analyses, we may need to convert the dependent variable “Diabetes_binary” to a factor.\n\n\nClean up data - before data manipulation (i.e. converting Diabetes_binary to a factor), I would like to identify if any rows include N/A values.\n\n\nNA_vec &lt;- c(seq_along(diabetes))\nnames(NA_vec) &lt;- names(diabetes)\n\nfor (i in seq_along(diabetes)) {\n  NA_vec[i] &lt;- sum(is.na(diabetes[i]))\n}\n\nNA_vec\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\n\nThere are no NAs in any of the columns, so no omitting of rows for that particular reason is required.\n\n\nOne unique challenge or assessment of this data set is that all variables except for BMI are binary and/or classes. They are all double/numeric type columns with integers denoting a factored response to an associated question, whether it be a binary Yes/No, or a grouped category like education level (with levels 1-6) With that said, we can get som summary statistics and distribution information around the only continuous variable: BMI.\n\n\nb &lt;- ggplot(data = diabetes, aes(x=BMI))\nb + geom_histogram(binwidth = 5) +\n  labs(title = \"BMI Histogram\") +\n  geom_vline(aes(xintercept = mean(BMI)),col='black',size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nGiven the number of class variables (especially binary), in the data set, determining the mean value for each column will give a proportional insight to each variable. For example, ~14% of the observations are either diabetic or prediabetic, while only ~4% of the respondents had experienced a stroke, and ~44% were considered smokers.\n\n\n(sum_stats &lt;- data.frame(lapply(diabetes, FUN = mean)))\n\n  Diabetes_binary    HighBP  HighChol CholCheck      BMI    Smoker    Stroke\n1        0.139333 0.4290011 0.4241209 0.9626695 28.38236 0.4431686 0.0405708\n  HeartDiseaseorAttack PhysActivity    Fruits   Veggies HvyAlcoholConsump\n1           0.09418559    0.7565437 0.6342558 0.8114199        0.05619678\n  AnyHealthcare NoDocbcCost  GenHlth MentHlth PhysHlth  DiffWalk       Sex\n1     0.9510525  0.08417692 2.511392 3.184772 4.242081 0.1682237 0.4403422\n       Age Education   Income\n1 8.032119  5.050434 6.053875\n\n\n\n\n\n\nTo support the model development, I will convert the target variable: Diabetes_binary to a factor column with status of “No” and “Yes”. Yes indicates someone who is either prediabetic or diabetic.\n\n\ndiab2 &lt;- diabetes |&gt;\n  mutate(DiabetesStatus = \n           ifelse(Diabetes_binary == 0, \"No\",\n                  ifelse(Diabetes_binary == 1, \"Yes\",\n                         \"ERROR\")))\ndiab2$DiabetesStatus &lt;- as.factor(diab2$DiabetesStatus)\n\nhead(diab2)\n\n# A tibble: 6 × 23\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 16 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;,\n#   DiabetesStatus &lt;fct&gt;\n\n\n\nIt can be somewhat difficult to visualize class variables. For several of the variables in the data set, I will plot the proportion of Diabetes incidence against each factor level, including sizing of each plot point based on the count of factor level occurrences. Plots were generated for Age, Income, and Education class variables.\n\n\ndistSum &lt;- diab2 |&gt;\n  group_by(Age) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(distSum, aes(x=Age, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n)) +\n  labs(title = \"Proportion of Individuals w/Diabetes per Age Group\")\n\n\n\n\n\n\n\n\n\nInc_Prop &lt;- diab2 |&gt;\n  group_by(Income) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(Inc_Prop, aes(x=Income, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n)) +\n  labs(title = \"Proportion of Individuals w/Diabetes per Income Level\")\n\n\n\n\n\n\n\n\n\nInc_Prop &lt;- diab2 |&gt;\n  group_by(Education) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(Inc_Prop, aes(x=Education, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n)) +\n  labs(title = \"Proportion of Individuals w/Diabetes per Education Level\")\n\n\n\n\n\n\n\n\n\nUsing a similar approach as above, a scatterplot-like graph is generated to evaluate BMI against diabetes incidence proportion. As with the plots above, the goal is to identify visually if there is a possible relationship between several class variables and the proportion of affirmative diabetes responses prior to generating fit models for the data set.\n\n\nBMI_Prop &lt;- diab2 |&gt;\n  group_by(BMI) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(BMI_Prop, aes(x=BMI, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n))\n\n\n\n\n\n\n\n\n\nPrior to beginning the modeling activities for this data set, I’ll save the manipulated diabetes dataset such that it can be read-in for modeling.\n\n\nsaveRDS(diab2, file = \"diab2.rds\")\n\n\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling Quarto File",
    "section": "",
    "text": "This is the Modeling file which will be used to construct and evaluate different model fits for predictive modeling of Diabetes Health Indicators.\n\n\n\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) conducts yearly phone surveys in the United States. The surveys include questions for Americans about their health status and care choices. Our data set is a subset of the BRFSS survey calls from 2015 which focus on diabetes-related survey responses. Below is a brief description of each of the variables included in the data set.\n\nDiabetes_binary: The dependent variable. A 0/1 associated with No/Yes response to “have you been told you have diabetes (or prediabetes)?\nHighBP: Predictor indicating an individual has been told they have high blood pressure.\nHighChol: Predictor indicating an individual has been told they have high cholesterol.\nCholCheck: Cholesterol check has occurred within the past 5 years.\nBMI: Body Mass Index; calculated as a ratio of weight to height.\nSmoker: Response to question, “Have you smoked at least 100 cigarettes in your entire life?\nStroke: Predictor indicating an individual has been told they experienced a stroke.\nHeartDiseaseorAttack: Individual has experienced heart disease or myocardial infarction.\nPhysActivity: Respondent has engaged in physical activity in the past 30 days.\nFruits: Respondent consumes fruit once or more per day.\nVeggies: Respondent consumes vegetables once or more per day.\nHvyAlcoholConsump: More than 14 drinks per week for men & more than 7 drinks per week for women.\nAnyHealthcare: Respondent has some kind of healtch coverage.\nNoDocbcCost: Response to the question, “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\nGenHlth: General scale (1-5) assessment of health\nMentHlth: Response to the question, “How many days during the past 30 was your mental health not good?”\nPhysHlth: Response to the question, “How many days during the past 30 was your physical health not good?”\nDiffWalk: Response to the question, “Do you have serious difficulty walking or climbing stairs?”\nSex: Sex identification 0/1::female/male\nAge: 13-level age category starting at 18 (i.e. level 1 is 18-24)\nEducation: Education scale 1-6 &gt; 1: Never attended school or only kindergarten &gt; 2: Grades 1-8 (elementary) &gt; 3: Grades 9-11 (some high school) &gt; 4: Grade 12 or GED (High school graduate) &gt; 5: College 1 to 3 years (some college) &gt; 6: College 4 years or more (college graduate)\nIncome: Income scale 1-8, ranging from 1 = less than 10,000 to 8 = 75,000 or more.\n\nI’ll start by activiting the necessary packages and reading in the requisite data set identified/explored previously (see link to EDA quarto document at bottom of page).\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(ranger)\nlibrary(Metrics)\n\ndiabmodel &lt;- readRDS(\"diab2.rds\")\n\n\nMy first step in generating models for prediction of diabetes incidence, I need to partition the data set into training & test sets. This includes setting a seed value such that the results can be reproducible.\n\n\nset.seed(90)\ntrainIndex &lt;- createDataPartition(diabmodel$DiabetesStatus, p = 0.7, list = FALSE)\ndiabTrain &lt;- diabmodel[trainIndex, ]\ndiabTest &lt;- diabmodel[-trainIndex, ]\n\n\nA training object will be created to support the subsequent training activities. This training method uses cross-validation with 5 subsets and uses the logLoss function to evaluate model effectiveness.\n\n\ntrctrl &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE,\n                       summaryFunction = mnLogLoss)\nset.seed(56)\n\n\n\n\nThree Generalized Linear Models (GLMs) will be evaluated with different combinations of predictors. A GLM is used when the response / dependent variable cannot be predicted via simply ordinary linear regression. In this case, we are evaluating a binary response, so a GLM is appropriate.\n\n\n\n\nStarting out with an evaluation of Age, BMI, High Blood Pressure, and High Cholesterol, along with their interaction effects.\n\n\n(glmFit1 &lt;- train(DiabetesStatus ~ Age*BMI*HighBP*HighChol,\n                 data = diabTrain,\n                 method = \"glm\",\n                 metric = \"logLoss\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n177577 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3400812\n\n\n\n\n\n\nNext use a model incorporating Age, BMI, Education, Income, Physical Activity, Smoker status, and their interaction effects.\n\n\n(glmFit2 &lt;- train(DiabetesStatus ~\n                    Age*BMI*Education*Income*PhysActivity*Smoker,\n                 data = diabTrain,\n                 method = \"glm\",\n                 metric = \"logLoss\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n177577 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3527875\n\n\n\n\n\n\nFinally, a model incorporating Education, Income, High Blood Pressure, High Cholesterol, Sex, and their interaction effects:\n\n\n(glmFit3 &lt;- train(DiabetesStatus ~\n                    Education*Income*HighBP*HighChol*Sex,\n                 data = diabTrain,\n                 method = \"glm\",\n                 metric = \"logLoss\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142062, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3515092\n\n\n\nEvaluating the lowest logLoss function result from the 3 models above identifies the best GLM model. For my purposes, the best model of the 3 was the first fit, which analyzed Age, BMI, High Blood Pressure, High Cholesterol, and their interaction effects.\n\n\n\n\n\n\nThe next model type I’ll explore is a classification tree. This model creates splits for each variable and generates a “tree” based on the various splits, and subsequent splits of data values (or between factors). Unlike the GLM models, where I would generate difference models with different predictors, classification trees have tuning parameters. The training activity below is used to determine the best tuning parameter (“cp”) value to use for the model.\n\n\n(treeFit1 &lt;- train(DiabetesStatus ~\nBMI+GenHlth+Smoker+Education+Income+Age+PhysActivity,\n                  data = diabmodel,\n                  method = \"rpart\",\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  tuneGrid = data.frame(cp = seq(0, 0.1,\n                                                 by = 0.001)),\n                  metric = \"logLoss\"))\n\nCART \n\n253680 samples\n     7 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (7), scaled (7) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202944, 202943, 202945, 202944 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3605379\n  0.001  0.3570110\n  0.002  0.3571636\n  0.003  0.4037509\n  0.004  0.4037509\n  0.005  0.4037509\n  0.006  0.4037509\n  0.007  0.4037509\n  0.008  0.4037509\n  0.009  0.4037509\n  0.010  0.4037509\n  0.011  0.4037509\n  0.012  0.4037509\n  0.013  0.4037509\n  0.014  0.4037509\n  0.015  0.4037509\n  0.016  0.4037509\n  0.017  0.4037509\n  0.018  0.4037509\n  0.019  0.4037509\n  0.020  0.4037509\n  0.021  0.4037509\n  0.022  0.4037509\n  0.023  0.4037509\n  0.024  0.4037509\n  0.025  0.4037509\n  0.026  0.4037509\n  0.027  0.4037509\n  0.028  0.4037509\n  0.029  0.4037509\n  0.030  0.4037509\n  0.031  0.4037509\n  0.032  0.4037509\n  0.033  0.4037509\n  0.034  0.4037509\n  0.035  0.4037509\n  0.036  0.4037509\n  0.037  0.4037509\n  0.038  0.4037509\n  0.039  0.4037509\n  0.040  0.4037509\n  0.041  0.4037509\n  0.042  0.4037509\n  0.043  0.4037509\n  0.044  0.4037509\n  0.045  0.4037509\n  0.046  0.4037509\n  0.047  0.4037509\n  0.048  0.4037509\n  0.049  0.4037509\n  0.050  0.4037509\n  0.051  0.4037509\n  0.052  0.4037509\n  0.053  0.4037509\n  0.054  0.4037509\n  0.055  0.4037509\n  0.056  0.4037509\n  0.057  0.4037509\n  0.058  0.4037509\n  0.059  0.4037509\n  0.060  0.4037509\n  0.061  0.4037509\n  0.062  0.4037509\n  0.063  0.4037509\n  0.064  0.4037509\n  0.065  0.4037509\n  0.066  0.4037509\n  0.067  0.4037509\n  0.068  0.4037509\n  0.069  0.4037509\n  0.070  0.4037509\n  0.071  0.4037509\n  0.072  0.4037509\n  0.073  0.4037509\n  0.074  0.4037509\n  0.075  0.4037509\n  0.076  0.4037509\n  0.077  0.4037509\n  0.078  0.4037509\n  0.079  0.4037509\n  0.080  0.4037509\n  0.081  0.4037509\n  0.082  0.4037509\n  0.083  0.4037509\n  0.084  0.4037509\n  0.085  0.4037509\n  0.086  0.4037509\n  0.087  0.4037509\n  0.088  0.4037509\n  0.089  0.4037509\n  0.090  0.4037509\n  0.091  0.4037509\n  0.092  0.4037509\n  0.093  0.4037509\n  0.094  0.4037509\n  0.095  0.4037509\n  0.096  0.4037509\n  0.097  0.4037509\n  0.098  0.4037509\n  0.099  0.4037509\n  0.100  0.4037509\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\n\nWith a logLoss of ~0.356, the best tuning parameter value is cp = 0.\n\n\n\n\n\nRandom Forest is an ensemble learning method which generates many different classification trees. For binomial responses like our data set, the output is determined by identifying the most prevalent outcome from all the different classification trees generated during the model training.\n\n\n(treeFit2 &lt;- train(DiabetesStatus ~ .,\n                  data = diabmodel,\n                  method = \"ranger\",\n                  tuneGrid = expand.grid(\n                    mtry = 2,\n                    splitrule = \"extratrees\",\n                    min.node.size=100),\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  metric = \"logLoss\",\n                  num.trees = 100))\n\nRandom Forest \n\n253680 samples\n    22 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202943, 202944, 202945, 202944 \nResampling results:\n\n  logLoss   \n  0.07103378\n\nTuning parameter 'mtry' was held constant at a value of 2\nTuning\n parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\n\n\n\nUsing this random forest method and the ‘ranger’ package which randomly select supporting parameters, the value of ‘mtry’ for a random forest model which minimizes logLoss value is mtry = 2.\n\n\n\n\n\nLet’s select the best model based on using the various training methodologies on the test set. The three models in question are the best results from the Logistic Regression, Classification Tree, and Random Forest models. For Logistic Regression, the lowest logLoss function result was the formula analyzing Age, BMI, High Blood Pressure, High Cholesterol, and their interaction effects. For Classification Tree - with predictors of Age, BMI, High Blood Pressure, High Cholesterol, Smoker, Education, Income, Age, and Physical Activity., the lowest logLoss function result was associated with the cp (complexity parameter) of 0.001. For The Random Forest, the lowest logLoss function result was associated with an all predictors included and an mtry of 2.\n\n\nTo compare the models further, we need to predict values using each model on the test set\n\n\n\n\nhead(glmpred &lt;- predict(glmFit1, diabTest, type = \"prob\"))\n\n         No        Yes\n1 0.5283996 0.47160036\n2 0.7481845 0.25181547\n3 0.8409993 0.15900073\n4 0.6222792 0.37772078\n5 0.9685681 0.03143189\n6 0.9287960 0.07120399\n\n\n\n(logLoss(diabmodel$Diabetes_binary, glmpred$Yes))\n\n[1] 0.4841449\n\n\n\n\n\n\nhead(ctpred &lt;- predict(treeFit1, diabTest, type = \"prob\", cp = 0.001))\n\n         No       Yes\n1 0.3927025 0.6072975\n2 0.8193585 0.1806415\n3 0.8193585 0.1806415\n4 0.6730328 0.3269672\n5 0.8193585 0.1806415\n6 0.8250890 0.1749110\n\n\n\n(logLoss(diabmodel$Diabetes_binary, ctpred$Yes))\n\n[1] 0.4511453\n\n\n\n\n\n\nhead(rfpred &lt;- predict(treeFit2, diabTest, type = \"prob\"))\n\n         No        Yes\n1 0.8944415 0.10555849\n2 0.9009731 0.09902686\n3 0.9644001 0.03559992\n4 0.8894971 0.11050286\n5 0.9780418 0.02195821\n6 0.9302189 0.06978107\n\n\n\nlogLoss(diabmodel$Diabetes_binary, rfpred$Yes)\n\n[1] 0.6738778\n\n\n\n\n\n\n\nThe final logLoss results from each of the 3 best iterations of each model, result in the following:\n\n\n\n\nModel\nlogLoss\n\n\n\n\nGLM\n0.4841\n\n\nClassification Tree\n0.4576\n\n\nRandom Forest\n0.6739\n\n\n\n\n\n\n\nPrior to populating the API file, I want to develop a function that will take in predictor values for arguments and produce a predicted value.\n\n\n#BMI*GenHlth*Smoker*Education*Income*Age*PhysActivity\n(predvec &lt;- data.frame(BMI = 25, GenHlth = 3, Smoker = 1,\n                       Education = 3, Income = 4, Age = 9,\n                       PhysActivity = 0))\n\n  BMI GenHlth Smoker Education Income Age PhysActivity\n1  25       3      1         3      4   9            0\n\n\n\npredict(treeFit1, newdata = predvec, type = \"prob\")\n\n         No       Yes\n1 0.8193585 0.1806415\n\n\n\nNow, using the structure above, I’ll develop a function building the newdata dataframe from the predictor variables specified.\n\n\npredct &lt;- function(BMI = 25, GenHlth = 3, Smoker = 1,\n                     Education = 4, Income = 5, Age = 8,\n                     PhysActivity = 1) {\n  pred_df &lt;- data.frame(BMI = BMI, GenHlth = GenHlth,\n                        Smoker = Smoker, Education = Education,\n                        Income = Income, Age = Age,\n                        PhysActivity = PhysActivity)\n  predict(treeFit1, newdata = pred_df, type = \"prob\")\n}\n\n\npredct(50, 2, 0, 6, 8, 10, 0)\n\n         No       Yes\n1 0.9440331 0.0559669\n\n\n\n\n\n\nUnfortunately, the best model (by logLoss value) consumes quite a few resources, and generating the model in an API file, hosted by a docker container, caused the docker image to time out before the container became active/usable. To ensure the exercise can be completed through a docker-based API, I will use a simpler model: GLM with only 2 predictors: Age & Income.\n\n\n(glmFit4 &lt;- train(DiabetesStatus ~ Age*Income,\n                  data = diabTrain,\n                  method = \"glm\",\n                  metric = \"logLoss\",\n                  trControl=trctrl))\n\nGeneralized Linear Model \n\n177577 samples\n     2 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142063, 142061, 142061, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3767495\n\n\n\n#Function for simplified GLM model\npredglm &lt;- function(Age = 8, Income = 5) {\n  pred_df &lt;- data.frame(Age = as.numeric(Age),\n                        Income = as.numeric(Income))\n  predict(glmFit4, newdata = pred_df, type = \"prob\")\n}\n\n\npredglm(Age = 11, Income = 2)\n\n         No       Yes\n1 0.6966121 0.3033879\n\n\n\n\n\nClick here for the EDA Page"
  },
  {
    "objectID": "EDA.html#introduction-to-data",
    "href": "EDA.html#introduction-to-data",
    "title": "EDA Quarto File",
    "section": "",
    "text": "The Behavioral Risk Factor Surveillance System (BRFSS) conducts yearly phone surveys in the United States. The surveys include questions for Americans about their health status and care choices. Our data set is a subset of the BRFSS survey calls from 2015 which focus on diabetes-related survey responses. Below is a brief description of each of the variables included in the data set.\n\nDiabetes_binary: The dependent variable. A 0/1 associated with No/Yes response to “have you been told you have diabetes (or prediabetes)?\nHighBP: Predictor indicating an individual has been told they have high blood pressure.\nHighChol: Predictor indicating an individual has been told they have high cholesterol.\nCholCheck: Cholesterol check has occurred within the past 5 years.\nBMI: Body Mass Index; calculated as a ratio of weight to height.\nSmoker: Response to question, “Have you smoked at least 100 cigarettes in your entire life?\nStroke: Predictor indicating an individual has been told they experienced a stroke.\nHeartDiseaseorAttack: Individual has experienced heart disease or myocardial infarction.\nPhysActivity: Respondent has engaged in physical activity in the past 30 days.\nFruits: Respondent consumes fruit once or more per day.\nVeggies: Respondent consumes vegetables once or more per day.\nHvyAlcoholConsump: More than 14 drinks per week for men & more than 7 drinks per week for women.\nAnyHealthcare: Respondent has some kind of healtch coverage.\nNoDocbcCost: Response to the question, “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\nGenHlth: General scale (1-5) assessment of health\nMentHlth: Response to the question, “How many days during the past 30 was your mental health not good?”\nPhysHlth: Response to the question, “How many days during the past 30 was your physical health not good?”\nDiffWalk: Response to the question, “Do you have serious difficulty walking or climbing stairs?”\nSex: Sex identification 0/1::female/male\nAge: 13-level age category starting at 18 (i.e. level 1 is 18-24)\nEducation: Education scale 1-6 &gt; 1: Never attended school or only kindergarten &gt; 2: Grades 1-8 (elementary) &gt; 3: Grades 9-11 (some high school) &gt; 4: Grade 12 or GED (High school graduate) &gt; 5: College 1 to 3 years (some college) &gt; 6: College 4 years or more (college graduate)\nIncome: Income scale 1-8, ranging from 1 = less than 10,000 to 8 = 75,000 or more.\n\n\n\nI will begin my data exploration by activiting the necessary packages\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nWe will read in the data to a dataframe object, and purposefully identify the data type for each of the 22 variables.\n\n\ndiabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspec(diabetes)\n\ncols(\n  Diabetes_binary = col_double(),\n  HighBP = col_double(),\n  HighChol = col_double(),\n  CholCheck = col_double(),\n  BMI = col_double(),\n  Smoker = col_double(),\n  Stroke = col_double(),\n  HeartDiseaseorAttack = col_double(),\n  PhysActivity = col_double(),\n  Fruits = col_double(),\n  Veggies = col_double(),\n  HvyAlcoholConsump = col_double(),\n  AnyHealthcare = col_double(),\n  NoDocbcCost = col_double(),\n  GenHlth = col_double(),\n  MentHlth = col_double(),\n  PhysHlth = col_double(),\n  DiffWalk = col_double(),\n  Sex = col_double(),\n  Age = col_double(),\n  Education = col_double(),\n  Income = col_double()\n)\n\n\n\nAll data types are “doubles”, but for some analyses, we may need to convert the dependent variable “Diabetes_binary” to a factor.\n\n\nClean up data - before data manipulation (i.e. converting Diabetes_binary to a factor), I would like to identify if any rows include N/A values.\n\n\nNA_vec &lt;- c(seq_along(diabetes))\nnames(NA_vec) &lt;- names(diabetes)\n\nfor (i in seq_along(diabetes)) {\n  NA_vec[i] &lt;- sum(is.na(diabetes[i]))\n}\n\nNA_vec\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\n\nThere are no NAs in any of the columns, so no omitting of rows for that particular reason is required.\n\n\nOne unique challenge or assessment of this data set is that all variables except for BMI are binary and/or classes. They are all double/numeric type columns with integers denoting a factored response to an associated question, whether it be a binary Yes/No, or a grouped category like education level (with levels 1-6) With that said, we can get som summary statistics and distribution information around the only continuous variable: BMI.\n\n\nb &lt;- ggplot(data = diabetes, aes(x=BMI))\nb + geom_histogram(binwidth = 5) +\n  labs(title = \"BMI Histogram\") +\n  geom_vline(aes(xintercept = mean(BMI)),col='black',size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nGiven the number of class variables (especially binary), in the data set, determining the mean value for each column will give a proportional insight to each variable. For example, ~14% of the observations are either diabetic or prediabetic, while only ~4% of the respondents had experienced a stroke, and ~44% were considered smokers.\n\n\n(sum_stats &lt;- data.frame(lapply(diabetes, FUN = mean)))\n\n  Diabetes_binary    HighBP  HighChol CholCheck      BMI    Smoker    Stroke\n1        0.139333 0.4290011 0.4241209 0.9626695 28.38236 0.4431686 0.0405708\n  HeartDiseaseorAttack PhysActivity    Fruits   Veggies HvyAlcoholConsump\n1           0.09418559    0.7565437 0.6342558 0.8114199        0.05619678\n  AnyHealthcare NoDocbcCost  GenHlth MentHlth PhysHlth  DiffWalk       Sex\n1     0.9510525  0.08417692 2.511392 3.184772 4.242081 0.1682237 0.4403422\n       Age Education   Income\n1 8.032119  5.050434 6.053875\n\n\n\n\n\n\nTo support the model development, I will convert the target variable: Diabetes_binary to a factor column with status of “No” and “Yes”. Yes indicates someone who is either prediabetic or diabetic.\n\n\ndiab2 &lt;- diabetes |&gt;\n  mutate(DiabetesStatus = \n           ifelse(Diabetes_binary == 0, \"No\",\n                  ifelse(Diabetes_binary == 1, \"Yes\",\n                         \"ERROR\")))\ndiab2$DiabetesStatus &lt;- as.factor(diab2$DiabetesStatus)\n\nhead(diab2)\n\n# A tibble: 6 × 23\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 16 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;,\n#   DiabetesStatus &lt;fct&gt;\n\n\n\nIt can be somewhat difficult to visualize class variables. For several of the variables in the data set, I will plot the proportion of Diabetes incidence against each factor level, including sizing of each plot point based on the count of factor level occurrences. Plots were generated for Age, Income, and Education class variables.\n\n\ndistSum &lt;- diab2 |&gt;\n  group_by(Age) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(distSum, aes(x=Age, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n)) +\n  labs(title = \"Proportion of Individuals w/Diabetes per Age Group\")\n\n\n\n\n\n\n\n\n\nInc_Prop &lt;- diab2 |&gt;\n  group_by(Income) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(Inc_Prop, aes(x=Income, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n)) +\n  labs(title = \"Proportion of Individuals w/Diabetes per Income Level\")\n\n\n\n\n\n\n\n\n\nInc_Prop &lt;- diab2 |&gt;\n  group_by(Education) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(Inc_Prop, aes(x=Education, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n)) +\n  labs(title = \"Proportion of Individuals w/Diabetes per Education Level\")\n\n\n\n\n\n\n\n\n\nUsing a similar approach as above, a scatterplot-like graph is generated to evaluate BMI against diabetes incidence proportion. As with the plots above, the goal is to identify visually if there is a possible relationship between several class variables and the proportion of affirmative diabetes responses prior to generating fit models for the data set.\n\n\nBMI_Prop &lt;- diab2 |&gt;\n  group_by(BMI) |&gt;\n  summarize(propDiabetes = mean(Diabetes_binary),\n            n = n()) |&gt;\n  ungroup()\nggplot(BMI_Prop, aes(x=BMI, y=propDiabetes)) +\n  geom_point(stat = \"identity\", aes(size = n))\n\n\n\n\n\n\n\n\n\nPrior to beginning the modeling activities for this data set, I’ll save the manipulated diabetes dataset such that it can be read-in for modeling.\n\n\nsaveRDS(diab2, file = \"diab2.rds\")\n\n\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html#introduction-to-data",
    "href": "Modeling.html#introduction-to-data",
    "title": "Modeling Quarto File",
    "section": "",
    "text": "The Behavioral Risk Factor Surveillance System (BRFSS) conducts yearly phone surveys in the United States. The surveys include questions for Americans about their health status and care choices. Our data set is a subset of the BRFSS survey calls from 2015 which focus on diabetes-related survey responses. Below is a brief description of each of the variables included in the data set.\n\nDiabetes_binary: The dependent variable. A 0/1 associated with No/Yes response to “have you been told you have diabetes (or prediabetes)?\nHighBP: Predictor indicating an individual has been told they have high blood pressure.\nHighChol: Predictor indicating an individual has been told they have high cholesterol.\nCholCheck: Cholesterol check has occurred within the past 5 years.\nBMI: Body Mass Index; calculated as a ratio of weight to height.\nSmoker: Response to question, “Have you smoked at least 100 cigarettes in your entire life?\nStroke: Predictor indicating an individual has been told they experienced a stroke.\nHeartDiseaseorAttack: Individual has experienced heart disease or myocardial infarction.\nPhysActivity: Respondent has engaged in physical activity in the past 30 days.\nFruits: Respondent consumes fruit once or more per day.\nVeggies: Respondent consumes vegetables once or more per day.\nHvyAlcoholConsump: More than 14 drinks per week for men & more than 7 drinks per week for women.\nAnyHealthcare: Respondent has some kind of healtch coverage.\nNoDocbcCost: Response to the question, “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\nGenHlth: General scale (1-5) assessment of health\nMentHlth: Response to the question, “How many days during the past 30 was your mental health not good?”\nPhysHlth: Response to the question, “How many days during the past 30 was your physical health not good?”\nDiffWalk: Response to the question, “Do you have serious difficulty walking or climbing stairs?”\nSex: Sex identification 0/1::female/male\nAge: 13-level age category starting at 18 (i.e. level 1 is 18-24)\nEducation: Education scale 1-6 &gt; 1: Never attended school or only kindergarten &gt; 2: Grades 1-8 (elementary) &gt; 3: Grades 9-11 (some high school) &gt; 4: Grade 12 or GED (High school graduate) &gt; 5: College 1 to 3 years (some college) &gt; 6: College 4 years or more (college graduate)\nIncome: Income scale 1-8, ranging from 1 = less than 10,000 to 8 = 75,000 or more.\n\nI’ll start by activiting the necessary packages and reading in the requisite data set identified/explored previously (see link to EDA quarto document at bottom of page).\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(ranger)\nlibrary(Metrics)\n\ndiabmodel &lt;- readRDS(\"diab2.rds\")\n\n\nMy first step in generating models for prediction of diabetes incidence, I need to partition the data set into training & test sets. This includes setting a seed value such that the results can be reproducible.\n\n\nset.seed(90)\ntrainIndex &lt;- createDataPartition(diabmodel$DiabetesStatus, p = 0.7, list = FALSE)\ndiabTrain &lt;- diabmodel[trainIndex, ]\ndiabTest &lt;- diabmodel[-trainIndex, ]\n\n\nA training object will be created to support the subsequent training activities. This training method uses cross-validation with 5 subsets and uses the logLoss function to evaluate model effectiveness.\n\n\ntrctrl &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE,\n                       summaryFunction = mnLogLoss)\nset.seed(56)\n\n\n\n\nThree Generalized Linear Models (GLMs) will be evaluated with different combinations of predictors. A GLM is used when the response / dependent variable cannot be predicted via simply ordinary linear regression. In this case, we are evaluating a binary response, so a GLM is appropriate.\n\n\n\n\nStarting out with an evaluation of Age, BMI, High Blood Pressure, and High Cholesterol, along with their interaction effects.\n\n\n(glmFit1 &lt;- train(DiabetesStatus ~ Age*BMI*HighBP*HighChol,\n                 data = diabTrain,\n                 method = \"glm\",\n                 metric = \"logLoss\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n177577 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3400812\n\n\n\n\n\n\nNext use a model incorporating Age, BMI, Education, Income, Physical Activity, Smoker status, and their interaction effects.\n\n\n(glmFit2 &lt;- train(DiabetesStatus ~\n                    Age*BMI*Education*Income*PhysActivity*Smoker,\n                 data = diabTrain,\n                 method = \"glm\",\n                 metric = \"logLoss\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n177577 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3527875\n\n\n\n\n\n\nFinally, a model incorporating Education, Income, High Blood Pressure, High Cholesterol, Sex, and their interaction effects:\n\n\n(glmFit3 &lt;- train(DiabetesStatus ~\n                    Education*Income*HighBP*HighChol*Sex,\n                 data = diabTrain,\n                 method = \"glm\",\n                 metric = \"logLoss\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142062, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3515092\n\n\n\nEvaluating the lowest logLoss function result from the 3 models above identifies the best GLM model. For my purposes, the best model of the 3 was the first fit, which analyzed Age, BMI, High Blood Pressure, High Cholesterol, and their interaction effects.\n\n\n\n\n\n\nThe next model type I’ll explore is a classification tree. This model creates splits for each variable and generates a “tree” based on the various splits, and subsequent splits of data values (or between factors). Unlike the GLM models, where I would generate difference models with different predictors, classification trees have tuning parameters. The training activity below is used to determine the best tuning parameter (“cp”) value to use for the model.\n\n\n(treeFit1 &lt;- train(DiabetesStatus ~\nBMI+GenHlth+Smoker+Education+Income+Age+PhysActivity,\n                  data = diabmodel,\n                  method = \"rpart\",\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  tuneGrid = data.frame(cp = seq(0, 0.1,\n                                                 by = 0.001)),\n                  metric = \"logLoss\"))\n\nCART \n\n253680 samples\n     7 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (7), scaled (7) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202944, 202943, 202945, 202944 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3605379\n  0.001  0.3570110\n  0.002  0.3571636\n  0.003  0.4037509\n  0.004  0.4037509\n  0.005  0.4037509\n  0.006  0.4037509\n  0.007  0.4037509\n  0.008  0.4037509\n  0.009  0.4037509\n  0.010  0.4037509\n  0.011  0.4037509\n  0.012  0.4037509\n  0.013  0.4037509\n  0.014  0.4037509\n  0.015  0.4037509\n  0.016  0.4037509\n  0.017  0.4037509\n  0.018  0.4037509\n  0.019  0.4037509\n  0.020  0.4037509\n  0.021  0.4037509\n  0.022  0.4037509\n  0.023  0.4037509\n  0.024  0.4037509\n  0.025  0.4037509\n  0.026  0.4037509\n  0.027  0.4037509\n  0.028  0.4037509\n  0.029  0.4037509\n  0.030  0.4037509\n  0.031  0.4037509\n  0.032  0.4037509\n  0.033  0.4037509\n  0.034  0.4037509\n  0.035  0.4037509\n  0.036  0.4037509\n  0.037  0.4037509\n  0.038  0.4037509\n  0.039  0.4037509\n  0.040  0.4037509\n  0.041  0.4037509\n  0.042  0.4037509\n  0.043  0.4037509\n  0.044  0.4037509\n  0.045  0.4037509\n  0.046  0.4037509\n  0.047  0.4037509\n  0.048  0.4037509\n  0.049  0.4037509\n  0.050  0.4037509\n  0.051  0.4037509\n  0.052  0.4037509\n  0.053  0.4037509\n  0.054  0.4037509\n  0.055  0.4037509\n  0.056  0.4037509\n  0.057  0.4037509\n  0.058  0.4037509\n  0.059  0.4037509\n  0.060  0.4037509\n  0.061  0.4037509\n  0.062  0.4037509\n  0.063  0.4037509\n  0.064  0.4037509\n  0.065  0.4037509\n  0.066  0.4037509\n  0.067  0.4037509\n  0.068  0.4037509\n  0.069  0.4037509\n  0.070  0.4037509\n  0.071  0.4037509\n  0.072  0.4037509\n  0.073  0.4037509\n  0.074  0.4037509\n  0.075  0.4037509\n  0.076  0.4037509\n  0.077  0.4037509\n  0.078  0.4037509\n  0.079  0.4037509\n  0.080  0.4037509\n  0.081  0.4037509\n  0.082  0.4037509\n  0.083  0.4037509\n  0.084  0.4037509\n  0.085  0.4037509\n  0.086  0.4037509\n  0.087  0.4037509\n  0.088  0.4037509\n  0.089  0.4037509\n  0.090  0.4037509\n  0.091  0.4037509\n  0.092  0.4037509\n  0.093  0.4037509\n  0.094  0.4037509\n  0.095  0.4037509\n  0.096  0.4037509\n  0.097  0.4037509\n  0.098  0.4037509\n  0.099  0.4037509\n  0.100  0.4037509\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\n\nWith a logLoss of ~0.356, the best tuning parameter value is cp = 0.\n\n\n\n\n\nRandom Forest is an ensemble learning method which generates many different classification trees. For binomial responses like our data set, the output is determined by identifying the most prevalent outcome from all the different classification trees generated during the model training.\n\n\n(treeFit2 &lt;- train(DiabetesStatus ~ .,\n                  data = diabmodel,\n                  method = \"ranger\",\n                  tuneGrid = expand.grid(\n                    mtry = 2,\n                    splitrule = \"extratrees\",\n                    min.node.size=100),\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  metric = \"logLoss\",\n                  num.trees = 100))\n\nRandom Forest \n\n253680 samples\n    22 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202943, 202944, 202945, 202944 \nResampling results:\n\n  logLoss   \n  0.07103378\n\nTuning parameter 'mtry' was held constant at a value of 2\nTuning\n parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\n\n\n\nUsing this random forest method and the ‘ranger’ package which randomly select supporting parameters, the value of ‘mtry’ for a random forest model which minimizes logLoss value is mtry = 2.\n\n\n\n\n\nLet’s select the best model based on using the various training methodologies on the test set. The three models in question are the best results from the Logistic Regression, Classification Tree, and Random Forest models. For Logistic Regression, the lowest logLoss function result was the formula analyzing Age, BMI, High Blood Pressure, High Cholesterol, and their interaction effects. For Classification Tree - with predictors of Age, BMI, High Blood Pressure, High Cholesterol, Smoker, Education, Income, Age, and Physical Activity., the lowest logLoss function result was associated with the cp (complexity parameter) of 0.001. For The Random Forest, the lowest logLoss function result was associated with an all predictors included and an mtry of 2.\n\n\nTo compare the models further, we need to predict values using each model on the test set\n\n\n\n\nhead(glmpred &lt;- predict(glmFit1, diabTest, type = \"prob\"))\n\n         No        Yes\n1 0.5283996 0.47160036\n2 0.7481845 0.25181547\n3 0.8409993 0.15900073\n4 0.6222792 0.37772078\n5 0.9685681 0.03143189\n6 0.9287960 0.07120399\n\n\n\n(logLoss(diabmodel$Diabetes_binary, glmpred$Yes))\n\n[1] 0.4841449\n\n\n\n\n\n\nhead(ctpred &lt;- predict(treeFit1, diabTest, type = \"prob\", cp = 0.001))\n\n         No       Yes\n1 0.3927025 0.6072975\n2 0.8193585 0.1806415\n3 0.8193585 0.1806415\n4 0.6730328 0.3269672\n5 0.8193585 0.1806415\n6 0.8250890 0.1749110\n\n\n\n(logLoss(diabmodel$Diabetes_binary, ctpred$Yes))\n\n[1] 0.4511453\n\n\n\n\n\n\nhead(rfpred &lt;- predict(treeFit2, diabTest, type = \"prob\"))\n\n         No        Yes\n1 0.8944415 0.10555849\n2 0.9009731 0.09902686\n3 0.9644001 0.03559992\n4 0.8894971 0.11050286\n5 0.9780418 0.02195821\n6 0.9302189 0.06978107\n\n\n\nlogLoss(diabmodel$Diabetes_binary, rfpred$Yes)\n\n[1] 0.6738778\n\n\n\n\n\n\n\nThe final logLoss results from each of the 3 best iterations of each model, result in the following:\n\n\n\n\nModel\nlogLoss\n\n\n\n\nGLM\n0.4841\n\n\nClassification Tree\n0.4576\n\n\nRandom Forest\n0.6739\n\n\n\n\n\n\n\nPrior to populating the API file, I want to develop a function that will take in predictor values for arguments and produce a predicted value.\n\n\n#BMI*GenHlth*Smoker*Education*Income*Age*PhysActivity\n(predvec &lt;- data.frame(BMI = 25, GenHlth = 3, Smoker = 1,\n                       Education = 3, Income = 4, Age = 9,\n                       PhysActivity = 0))\n\n  BMI GenHlth Smoker Education Income Age PhysActivity\n1  25       3      1         3      4   9            0\n\n\n\npredict(treeFit1, newdata = predvec, type = \"prob\")\n\n         No       Yes\n1 0.8193585 0.1806415\n\n\n\nNow, using the structure above, I’ll develop a function building the newdata dataframe from the predictor variables specified.\n\n\npredct &lt;- function(BMI = 25, GenHlth = 3, Smoker = 1,\n                     Education = 4, Income = 5, Age = 8,\n                     PhysActivity = 1) {\n  pred_df &lt;- data.frame(BMI = BMI, GenHlth = GenHlth,\n                        Smoker = Smoker, Education = Education,\n                        Income = Income, Age = Age,\n                        PhysActivity = PhysActivity)\n  predict(treeFit1, newdata = pred_df, type = \"prob\")\n}\n\n\npredct(50, 2, 0, 6, 8, 10, 0)\n\n         No       Yes\n1 0.9440331 0.0559669\n\n\n\n\n\n\nUnfortunately, the best model (by logLoss value) consumes quite a few resources, and generating the model in an API file, hosted by a docker container, caused the docker image to time out before the container became active/usable. To ensure the exercise can be completed through a docker-based API, I will use a simpler model: GLM with only 2 predictors: Age & Income.\n\n\n(glmFit4 &lt;- train(DiabetesStatus ~ Age*Income,\n                  data = diabTrain,\n                  method = \"glm\",\n                  metric = \"logLoss\",\n                  trControl=trctrl))\n\nGeneralized Linear Model \n\n177577 samples\n     2 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142063, 142061, 142061, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3767495\n\n\n\n#Function for simplified GLM model\npredglm &lt;- function(Age = 8, Income = 5) {\n  pred_df &lt;- data.frame(Age = as.numeric(Age),\n                        Income = as.numeric(Income))\n  predict(glmFit4, newdata = pred_df, type = \"prob\")\n}\n\n\npredglm(Age = 11, Income = 2)\n\n         No       Yes\n1 0.6966121 0.3033879\n\n\n\n\n\nClick here for the EDA Page"
  }
]