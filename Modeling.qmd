# Modeling Quarto File

> This is the Modeling file which will be used to construct and evaluate different model fits for predictive modeling of Diabetes Health Indicators.

> Start by activiting the necessary packages

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(caret)
library(ranger)
library(Metrics)

diabmodel <- readRDS("diab2.rds")
```

> Partition the data set into training & test sets

```{r}
set.seed(90)
trainIndex <- createDataPartition(diabmodel$DiabetesStatus, p = 0.7, list = FALSE)
diabTrain <- diabmodel[trainIndex, ]
diabTest <- diabmodel[-trainIndex, ]
```

> Begin modeling:

> Create the training object

```{r}
trctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE,
                       summaryFunction = mnLogLoss)
set.seed(56)
```

### Logistic Regression Models

#### Age, BMI, HighBP, HighChol
> Starting with a model only dependent upon age

```{r}
(glmFit1 <- train(DiabetesStatus ~ Age*BMI*HighBP*HighChol,
                 data = diabTrain,
                 method = "glm",
                 metric = "logLoss",
                 trControl=trctrl))
```

#### Age, BMI, Education, Income, Physical Activity, Smoker
> Next use a model incorporating age & BMI, with interaction effects:

```{r}
(glmFit2 <- train(DiabetesStatus ~
                    Age*BMI*Education*Income*PhysActivity*Smoker,
                 data = diabTrain,
                 method = "glm",
                 metric = "logLoss",
                 trControl=trctrl))
```

#### Education, Income, High BP, High Cholesterol
> Finally, a model incorporating Education, Income, and their interaction effects:

```{r}
(glmFit3 <- train(DiabetesStatus ~
                    Education*Income*HighBP*HighChol*Sex,
                 data = diabTrain,
                 method = "glm",
                 metric = "logLoss",
                 trControl=trctrl))
```

### Classification Tree

```{r}
(treeFit1 <- train(DiabetesStatus ~
BMI*GenHlth*Smoker*Education*Income*Age*PhysActivity,
                  data = diabmodel,
                  method = "rpart",
                  trControl=trctrl,
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cp = seq(0, 0.1,
                                                 by = 0.001)),
                  metric = "logLoss"))
```

### Random Forest

```{r}
(treeFit2 <- train(DiabetesStatus ~ .,
                  data = diabmodel,
                  method = "ranger",
                  tuneGrid = expand.grid(
                    mtry = 2,
                    splitrule = "extratrees",
                    min.node.size=100),
                  trControl=trctrl,
                  preProcess = c("center", "scale"),
                  metric = "logLoss",
                  num.trees = 100))
```

> Using this random forest method and the 'ranger' package which randomly select supporting parameters, the value of 'mtry' for a random forest model which minimizes logLoss value is mtry = 2.

### Final Model Selection

> Let's select the best model based on using the various training methodologies on the test set. The three models in question are the best results from the Logistic Regression, Classification Tree, and Random Forest models.
> For Logistic Regression, the lowest logLoss function result was the formula analyzing Age, BMI, High Blood Pressure, High Cholesterol, and their interaction effects.
> For Classification Tree - with predictors of BMI, General Health, and their interaction effects, the lowest logLoss function result was associated with the cp (complexity parameter) of 0.
> For The Random Forest, the lowest logLoss function result was associated with an mtry of 2.

> First, we need to predict values using each model on the test set

#### GLM Prediction

```{r}
head(glmpred <- predict(glmFit1, diabTest, type = "prob"))
```

```{r, warning=FALSE}
(logLoss(diabmodel$Diabetes_binary, glmpred$Yes))
```

#### Classification Tree Prediction

```{r}
head(ctpred <- predict(treeFit1, diabTest, type = "prob", cp = 0.001))
```

```{r, warning=FALSE}
(logLoss(diabmodel$Diabetes_binary, ctpred$Yes))
```

#### Random Forest Prediction

```{r}
head(rfpred <- predict(treeFit2, diabTest, type = "prob"))
```

```{r, warning=FALSE}
logLoss(diabmodel$Diabetes_binary, rfpred$Yes)
```

### Model Selection

> The final logLoss results from each of the 3 best iterations of each model, result in the following:

|          Model        |   logLoss   |
|----------------------:|:-----------:|
|           GLM         |   0.4841    |
|  Classification Tree  |   0.4576    |
|     Random Forest     |   0.6739    |

### Function for Prediction
> Prior to populating the API file, I want to develop a function that will take in predictor values for arguments and produce a predicted value.

```{r}
#BMI*GenHlth*Smoker*Education*Income*Age*PhysActivity
(predvec <- data.frame(BMI = 25, GenHlth = 3, Smoker = 1,
                       Education = 3, Income = 4, Age = 9,
                       PhysActivity = 0))
```

```{r}
predict(treeFit1, newdata = predvec, type = "prob")
```

> Now, using the structure above, I'll develop a function building the newdata dataframe from the predictor variables specified.

```{r}
predct <- function(BMI = 25, GenHlth = 3, Smoker = 1,
                     Education = 4, Income = 5, Age = 8,
                     PhysActivity = 1) {
  pred_df <- data.frame(BMI = BMI, GenHlth = GenHlth,
                        Smoker = Smoker, Education = Education,
                        Income = Income, Age = Age,
                        PhysActivity = PhysActivity)
  predict(treeFit1, newdata = pred_df, type = "prob")
}
```

```{r}
predct(50, 2, 0, 6, 8, 10, 0)
```

### Function for API Query

> Unfortunately, the best model (by logLoss value) consumes quite a few resources, and generating the model in an API file, hosted by a docker container, caused the docker image to time out before the container became active/usable. To ensure the exercise can be completed through a docker-based API, I will use a simpler model: GLM with only 2 predictors: Age & Income.

```{r}
(glmFit4 <- train(DiabetesStatus ~ Age*Income,
                  data = diabTrain,
                  method = "glm",
                  metric = "logLoss",
                  trControl=trctrl))
```

```{r}
#Function for simplified GLM model
predglm <- function(Age = 8, Income = 5) {
  pred_df <- data.frame(Age = as.numeric(Age),
                        Income = as.numeric(Income))
  predict(glmFit4, newdata = pred_df, type = "prob")
}
```

```{r}
predglm(Age = 11, Income = 2)
```

### Link to EDA Site:

[Click here for the EDA Page](EDA.html)